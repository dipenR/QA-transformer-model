# -*- coding: utf-8 -*-
"""QATransformer-v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17LThfa3BvYCYaLlFC_SZ22HND_lVgeGS
"""

import json
import numpy as np 
import pandas as pd

import torch
import torch.nn as nn
# from torch.autograd import Variable
import torch.nn.functional as F

# ! pip install datasets transformers # uncomment to install if needed
import transformers
# from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForQuestionAnswering, default_data_collator, TrainingArguments, Trainer, pipeline
from datasets import Dataset, load_metric

class BoolQA(nn.Module):

    def __init__(self, bert_model, freeze_bert=False):
        super(BoolQA, self).__init__()
        self.bert_layer = transformers.AutoModel.from_pretrained(bert_model)

        self.cls_layer = nn.Linear(1024, 2)
        self.dropout = nn.Dropout(0.1)

    def forward(self, input_ids, attention_mask, token_type_ids):
      cont_reps, pooler_output = self.bert_layer(input_ids, attention_mask, token_type_ids, return_dict=False)
      
      logits = self.cls_layer(self.dropout(pooler_output))
      
      return logits

class QATrainer(transformers.Trainer):
  def compute_loss(self, model, inputs, return_outputs=False):
    labels = inputs.pop('labels')
    outputs = model(**inputs)

    # print(f"{type(outputs)}{outputs.shape}")
    probs = F.softmax(outputs, dim=-1)
    
    loss = nn.BCEWithLogitsLoss(probs.t()[1], labels.t())
    return (loss, outputs) if return_outputs else loss

def preprocess_fn(data):
  if key_2 is None:
    return tokenizer(data[key_1], truncation=True)
  else:
    return tokenizer(data[key_1], data[key_2], truncation=True)

if __name__ == "__main__": 
  model_checkpoint = "deepset/bert-large-uncased-whole-word-masking-squad2"
  batch_size = 4
  loss_func = nn.BCEWithLogitsLoss()

  tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)

  train_data = []
  dev_data = []
  with open('music_QA_train.json', 'r') as infile:
      train_data = json.load(infile)
  with open('music_QA_dev.json', 'r') as infile:
      dev_data = json.load(infile)
    
  for rec in train_data:
    rec['label'] = 1. if rec['label'] else 0.
  for rec in dev_data:
    rec['label'] = 1. if rec['label'] else 0.

  train_dataset = Dataset.from_dict({k: [d[k] for d in train_data] for k in train_data[0]})
  dev_dataset = Dataset.from_dict({k: [d[k] for d in dev_data] for k in dev_data[0]})
  metric = load_metric('glue', 'qnli')

  key_1 = 'question'
  key_2 = 'passage'

  encoded_train_dataset=train_dataset.map(preprocess_fn, batched=True, remove_columns=['question', 'passage'])
  encoded_dev_dataset=dev_dataset.map(preprocess_fn, batched=True, remove_columns=['question', 'passage'])

  args = transformers.TrainingArguments(
      "Test",
      evaluation_strategy = "epoch",
      learning_rate=1e-5,
      per_device_train_batch_size=batch_size,
      per_device_eval_batch_size=batch_size,
      num_train_epochs=5,
      weight_decay=0.01,
  )

  model = BoolQA(model_checkpoint, False)

  trainer = QATrainer(
      model,
      args,
      train_dataset = encoded_train_dataset,
      eval_dataset = encoded_dev_dataset,
      tokenizer = tokenizer
  )

  trainer.train()

  test_data = []
  with open('music_QA_test.json', 'r') as infile:
      test_data = json.load(infile)

  answers = []
  i=0
  model.to(device='cuda:0') 
  for input in test_data:
    input = tokenizer(test_data[i]['question'], test_data[i]['passage'], truncation=True, return_tensors='pt')
    input.to(device='cuda:0')
    logits = model(**input)
    probs = F.softmax(logits, dim=-1)
    answers.append(probs.argmax().item())
    i+=1

  ids = []
  for d in test_data:
    ids.append(d['idx'])

  data = {'idx': ids, 'label': answers}
  df = pd.DataFrame(data=data)

  # uncomment this to generate a submmissions.csv file on the test set
  # df.to_csv('submissions.csv', index=False)

  answers = []
  i=0
  model.to(device='cuda:0') 
  
  for input in train_data:
    input = tokenizer(train_data[i]['question'], train_data[i]['passage'], truncation=True, return_tensors='pt')
    input.to(device='cuda:0')
    logits = model(**input)
    probs = F.softmax(logits, dim=-1)
    answers.append(probs.argmax().item())
    i+=1

  actual = [d['label'] for d in train_data]

  correct = 0
  for i in range(len(actual)):
    if actual[i] == answers[i]:
      correct+=1
  print(f"accuracy on training set: {correct/419}")
