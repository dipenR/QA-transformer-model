# -*- coding: utf-8 -*-
"""QATransformer-v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17LThfa3BvYCYaLlFC_SZ22HND_lVgeGS
"""

import json
import numpy as np 
import pandas as pd

import torch
import torch.nn as nn
# from torch.autograd import Variable
import torch.nn.functional as F

# ! pip install datasets transformers # uncomment to install if needed
import transformers
# from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForQuestionAnswering, default_data_collator, TrainingArguments, Trainer, pipeline
from datasets import Dataset, load_metric

# this class extents nn.module to implement a bert transformer. 
# It adds a linear layer to the given model-checkpoint that serves as the classifier output
class BoolQA(nn.Module):

    def __init__(self, bert_model, freeze_bert=False):
        super(BoolQA, self).__init__()
        self.bert_layer = transformers.AutoModel.from_pretrained(bert_model)

        self.cls_layer = nn.Linear(1024, 2)
        self.dropout = nn.Dropout(0.1)

    def forward(self, input_ids, attention_mask, token_type_ids):
      cont_reps, pooler_output = self.bert_layer(input_ids, attention_mask, token_type_ids, return_dict=False)
      
      logits = self.cls_layer(self.dropout(pooler_output))
      
      return logits
# This builds on top of the hugginface trainer lib and changes the compute loss method 
# to accomodate for BCEWithLogitsLoss.
class QATrainer(transformers.Trainer):
  def compute_loss(self, model, inputs, return_outputs=False):
    labels = inputs.pop('labels')
    outputs = model(**inputs)

    # print(f"{type(outputs)}{outputs.shape}")
    probs = F.softmax(outputs, dim=-1)
    
    loss = nn.BCEWithLogitsLoss(probs.t()[1], labels.t())
    return (loss, outputs) if return_outputs else loss

# tokenizer function that tokenizes data with the tokenizer given.
def preprocess_fn(data):
  if key_2 is None:
    return tokenizer(data[key_1], truncation=True)
  else:
    return tokenizer(data[key_1], data[key_2], truncation=True)

if __name__ == "__main__": 
  model_checkpoint = "deepset/bert-large-uncased-whole-word-masking-squad2"
  batch_size = 4
  loss_func = nn.BCEWithLogitsLoss()

  tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)

  train_data = []
  dev_data = []
  with open('music_QA_train.json', 'r') as infile:
      train_data = json.load(infile)
  with open('music_QA_dev.json', 'r') as infile:
      dev_data = json.load(infile)
    
  # here we convert tbhe label data from the file from True/False to 1./0. the 
  # '.' indicates a Float and allows for the creation of FloatTensors
  for rec in train_data:
    rec['label'] = 1. if rec['label'] else 0.
  for rec in dev_data:
    rec['label'] = 1. if rec['label'] else 0.

  train_dataset = Dataset.from_dict({k: [d[k] for d in train_data] for k in train_data[0]})
  dev_dataset = Dataset.from_dict({k: [d[k] for d in dev_data] for k in dev_data[0]})
  metric = load_metric('glue', 'qnli')

  key_1 = 'question'
  key_2 = 'passage'

  encoded_train_dataset=train_dataset.map(preprocess_fn, batched=True, remove_columns=['question', 'passage'])
  encoded_dev_dataset=dev_dataset.map(preprocess_fn, batched=True, remove_columns=['question', 'passage'])

  args = transformers.TrainingArguments(
      "BoolQA",
      evaluation_strategy = "epoch",
      learning_rate=1e-5,
      per_device_train_batch_size=batch_size,
      per_device_eval_batch_size=batch_size,
      num_train_epochs=5,
      weight_decay=0.01,
  )

  model = BoolQA(model_checkpoint, False)

  trainer = QATrainer(
      model,
      args,
      train_dataset = encoded_train_dataset,
      eval_dataset = encoded_dev_dataset,
      tokenizer = tokenizer
  )

  trainer.train()
  print("Training complete")

  model.to(device='cuda:0') 
  while(True):
    question = input("Enter a question: ")
    context = input("Enter a context: ") 
    
    input = tokenizer(question, context, truncation = True, return_tensors='pt')
    input.to(device='cuda:0')

    logits = model(**input)
    probs = F.softmax(logits, dim=-1)
    print(f"Question: {question}\n Context: {context}\n Answer = {probs.argmax().item()}")
    cont = input("Continue?[y/n]")
    if cont != 'y' or cont != 'Y':
      break
